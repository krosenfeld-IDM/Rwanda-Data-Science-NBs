{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mu3nKbKu07FD"
   },
   "source": [
    "# DAY 3: Generalization and Uncertainty\n",
    "\n",
    "\n",
    "### Machine Learning and Computational Statistics (DSC6232) \n",
    "#### Instructors: Weiwei Pan, Melanie Fernandez, Pavlos Protopapas \n",
    "#### Due: July 26th, 2:00 pm Kigali Time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWv2ocFVRfAD"
   },
   "source": [
    "**First name**: _________________________________________________________\n",
    "\n",
    "\n",
    "**Last name**: _____________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_Q7Fej559iU"
   },
   "source": [
    "## Learning Goals:\n",
    "\n",
    "1. explore the concept of high-variance: why is it a problem?\n",
    "2. understand confidence intervals and predictive intervals\n",
    "3. understand regularization using Ridge and Lasso regression\n",
    "4. understand bagging: training an ensemble of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyabblMA07FF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUBdYcQF07FJ"
   },
   "source": [
    "### We include helper functions here that we will need later (no need to read in detail!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l3AtqBBM07Fl"
   },
   "outputs": [],
   "source": [
    "def make_data(number_of_points, f):\n",
    "    '''\n",
    "    This function randomly samples training data in [-5, 5], given a function f.\n",
    "    \n",
    "    number_of_points: number of points to sample\n",
    "    f: the underlying function relating x and y\n",
    "    '''\n",
    "    #randomly sample training inputs from [-5, 5]\n",
    "    x = np.random.uniform(-5, 5, number_of_points)\n",
    "    #sort the randomly sampled training inputs\n",
    "    x = np.sort(x)\n",
    "    #generate noisy training labels for the sampled training inputs\n",
    "    y = f(x) + np.random.normal(0, 0.5, number_of_points)\n",
    "    return x, y\n",
    "\n",
    "def fit_polynomial_regression(x_train, y_train, x_test, polynomial_features, regression):\n",
    "    '''\n",
    "    This function implements polynomial regression, given training and testing data as well as\n",
    "    a polynomial feature model and a linear regression model.\n",
    "    \n",
    "    x_train: training inputs\n",
    "    y_train: training labels\n",
    "    x_test: tests inputs\n",
    "    polynomial_features: model for transforming inputs into polynomial features\n",
    "    regression: linear regression model\n",
    "    '''\n",
    "    #tranform training inputs into polynomial features\n",
    "    poly_x_train = polynomial_features.fit_transform(x_train.reshape((-1, 1)))\n",
    "    #tranform testing inputs into polynomial features\n",
    "    poly_x_test = polynomial_features.transform(x_test.reshape((-1, 1)))\n",
    "    #fit linear regression on top of polynomial features\n",
    "    regression.fit(poly_x_train, y_train)\n",
    "    #predict labels for test input\n",
    "    y_predict = regression.predict(poly_x_test)\n",
    "    return regression, y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tof3mHPF07Fk"
   },
   "source": [
    "## 1. The Effects of High-Variance\n",
    "\n",
    "In this section, we explore the properties of models that have high variance.\n",
    "\n",
    "Let's generate some very noisy data using a simple cubic function $f(x) = 0.01 x^3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "-tFPp6WF07Fq",
    "outputId": "3489faa2-4fd5-4b42-e9be-7c86ebe6c682"
   },
   "outputs": [],
   "source": [
    "#number of training inputs\n",
    "number_of_train_points = 50\n",
    "#number of testing inputs\n",
    "number_of_test_points = 100\n",
    "#function generating our data\n",
    "f = lambda x: 0.01 * x**3\n",
    "\n",
    "#make training data\n",
    "x_train, y_train = make_data(number_of_train_points, f)\n",
    "#make test input that is evenly spaced over [-5, 5]\n",
    "x_test = np.linspace(-5, 5, number_of_test_points)\n",
    "\n",
    "#visualize training data\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "#plot training data\n",
    "ax.scatter(x_train, y_train, color='black', label='training data')\n",
    "#plot the underlying function generating the data\n",
    "ax.plot(x_train, f(x_train), color='red', label='function generating data')\n",
    "ax.set_title('Training Data')\n",
    "ax.set_ylim([-2, 2])\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlzjkyHT07Ft"
   },
   "source": [
    "Now, let's fit polynomial regression models with different degrees to our training data. Higher polynomial degree means that the model can learn more complicated functions (more flexible). In the code below, we train the same model twice with slightly different data (resampled from the true generating function).\n",
    "\n",
    "**Exercise 1:**  Try training polynomial regression models with different degrees (some very low, e.g. degree=2, some very high, e.g. degree=20) - you need to change the degree in our code and experiment! Which type of polynomial model changes the most when we resample the training data? Which type of polynomial changes the least when we resample the training data?\n",
    "\n",
    "Why is this happening? Support your answers with concepts you've learned from lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "2JdFIwQ207Fu",
    "outputId": "29d3f03b-b7ab-481f-d4d3-db22ed414127"
   },
   "outputs": [],
   "source": [
    "#instantiate a polynomial feature model for a degree 20 polynomial\n",
    "polynomial_features_high = PolynomialFeatures(20)\n",
    "#instantiate a polynomial feature model for a degree 2 polynomial\n",
    "polynomial_features_low = PolynomialFeatures(2)\n",
    "#instantiate a linear regression model\n",
    "regression = LinearRegression(fit_intercept=False)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
    "colors = ['blue', 'red']\n",
    "\n",
    "#fit polynomial models on random samples of training sets\n",
    "for i in range(2):\n",
    "    #resample training data\n",
    "    x_train, y_train = make_data(number_of_train_points, f)\n",
    "    #fit our polynomial model on the resampled data\n",
    "    trained_regressor, y_predict = fit_polynomial_regression(x_train, y_train, x_test, polynomial_features_high, regression)\n",
    "    #plot the resampled training data\n",
    "    ax[0].scatter(x_train, y_train, color=colors[i], label='training data {}'.format(i))\n",
    "    #plot the fitted polynomial model\n",
    "    ax[0].plot(x_test, y_predict, color=colors[i], alpha=0.5, label='fitted model')\n",
    "    \n",
    "ax[0].set_title('Fitting a high degree polynomial on slightly different data')\n",
    "ax[0].set_ylim([-2, 2])\n",
    "ax[0].legend(loc='best')\n",
    "\n",
    "# fit low degree polynomial models\n",
    "for i in range(2):\n",
    "    #resample training data\n",
    "    x_train, y_train = make_data(number_of_train_points, f)\n",
    "    #fit our polynomial model on the resampled data\n",
    "    trained_regressor, y_predict = fit_polynomial_regression(x_train, y_train, x_test, polynomial_features_low, regression)\n",
    "    #plot the resampled training data\n",
    "    ax[1].scatter(x_train, y_train, color=colors[i], label='training data {}'.format(i))\n",
    "    #plot the fitted polynomial model\n",
    "    ax[1].plot(x_test, y_predict, color=colors[i], alpha=0.5, label='fitted model')\n",
    "    \n",
    "ax[1].set_title('Fitting a low degree polynomial on slightly different data')\n",
    "ax[1].set_ylim([-2, 2])\n",
    "ax[1].legend(loc='best')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6nRJM1hik92"
   },
   "source": [
    "**Exercise 2:** Which polynomial degree will yield the biggest difference in training and test MSE (you can either compute the training and test MSE or just reason about it), i.e. which model will overfit? Why?\n",
    "\n",
    "Is is a good idea to fit a degree 20 polynomial on this dataset? Why or why not? Support your answer with concepts you'ved learned in lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sz89qmvfjQeL"
   },
   "source": [
    "**Exercise 3:** Generate 100 random samples of training data. Use this to compute the 95% confidence intervals for the coefficients of the degree 20 polynomial model and plot them as box plots. Compute the 95% confidence intervals for the coefficients of the degree 2 polynomial model and plot them as box plots. \n",
    "\n",
    "Of the coefficients in the degree 20 polynomial model, which coefficients are we more certain about? Which ones are we less certain about?\n",
    "\n",
    "Overall, for which model are we more certain about the coefficients?\n",
    "\n",
    "Is this result what you expect? Support your answer using concepts you've learned in lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "Jsz1_WC0iCp4",
    "outputId": "5bb7bc61-e8a7-481a-a2a9-bab900f68efa"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "coefficients = [] # use this list to keep tract of the coefficients\n",
    "S = 100 # number of randomly sampled training data\n",
    "for i in range(S):\n",
    "    # resample training data\n",
    "    x_train, y_train = make_data(number_of_train_points, f)\n",
    "    # fit our polynomial model on the resampled data\n",
    "    trained_regressor, y_predict = fit_polynomial_regression(x_train, y_train, x_test, polynomial_features_low, regression)\n",
    "    # add the coefficients of the trained model to our list\n",
    "    coefficients.append(trained_regressor.coef_)\n",
    "# turn the list of coefficients into a numpy array\n",
    "coefficients = np.array(coefficients)\n",
    "# make box plots for each coefficient\n",
    "ax.boxplot(coefficients)\n",
    "\n",
    "ax.set_xticklabels([0, 1, 2])\n",
    "ax.grid()\n",
    "ax.set_title('Variations in the coefficients of a degree 2 polynomial model')\n",
    "ax.set_xlabel('Degree of term')\n",
    "ax.set_ylabel('Coefficient size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKkqQu7ysyI2"
   },
   "source": [
    "**Exercise 4**: Based on the confidence intervals of the model coefficients, which model will produce the most undertain predictions (i.e. have the widest prediction interval)? Why? You can answer this question by computing the 95% prediction interval or you can reason about the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DrwVSIy07Fy"
   },
   "source": [
    "## 2. Regularization\n",
    "In the following we will implement Ridge ($\\ell_2$ regularization) regression, and observe the effect of regularization on variance reduction.\n",
    "\n",
    "**Exercise 5:** repeat the experiment in **Exercise 1** for a degree 20 polynomial using Ridge regression. You should try different choices for the regularization parameter `alpha`. \n",
    "\n",
    "Visualize the Ridge polynomial regression model and discuss the impact of regularization on the fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "gQoSHF5M07Fz",
    "outputId": "fed36255-fc52-4814-da0f-b0f679e3a0e5"
   },
   "outputs": [],
   "source": [
    "# Step 0: instantiate a polynomial feature model for a degree 20 polynomial\n",
    "polynomial_features_high = PolynomialFeatures(20)\n",
    "\n",
    "# Step 1: instantiate a RidgeRegreession model\n",
    "# Hint: see documentation https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "# Remember to set fit_intercept=False\n",
    "# ridge_regression = ...\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "colors = ['blue', 'red']\n",
    "\n",
    "# Step 2: fit polynomial models on random samples of training sets\n",
    "for i in range(2):\n",
    "    #resample training data\n",
    "    x_train, y_train = make_data(number_of_train_points, f)\n",
    "\n",
    "    #fit our polynomial model on the resampled data\n",
    "#     trained_regressor, y_predict = fit_polynomial_regression(x_train, y_train, x_test, polynomial_features_high, ridge_regression)\n",
    "\n",
    "    #plot the resampled training data\n",
    "    ax.scatter(x_train, y_train, color=colors[i], label='training data {}'.format(i))\n",
    "\n",
    "    #plot the fitted polynomial model\n",
    "#     ax.plot(x_test, y_predict, color=colors[i], alpha=0.5, label='fitted model')\n",
    "    \n",
    "ax.set_title('Fitting a regularized high degree polynomial on slightly different data')\n",
    "ax.set_ylim([-2, 2])\n",
    "ax.legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sru1EXvM07F1"
   },
   "source": [
    "**Question**: What is the effect of regularization on this model's variance? Compare Ridge and Lasso regression: what differences do you observe? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gsUuD_y07F5"
   },
   "source": [
    "## 3. Bagging\n",
    "In the following we will implement bagging:\n",
    "1. we will train a large ensemble of complex models on different samples of training data\n",
    "2. we will average the predictions of the models in our ensemble to reduce the variance in the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNcJziKL07F5"
   },
   "source": [
    "**Exercise 6:** Repeat the experiment above (in Section 1) 100 times: that is, generate randomly sampled training data 100 times and train a degree 20 polynomial on each resampled training data set (this is our \"ensemble\" of models). To make predictions on the test set, we average the predictions of all the models in our ensemble.\n",
    "\n",
    "Visualize each model in the ensemble. Visualize also their average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "EjF2KQjy07F6",
    "outputId": "6de94e38-1be8-4a9f-d6cc-a6efb612a62d"
   },
   "outputs": [],
   "source": [
    "#instantiate a polynomial feature model for a degree 20 polynomial\n",
    "polynomial_features_high = PolynomialFeatures(20)\n",
    "\n",
    "#instantiate a linear regression model\n",
    "regression = LinearRegression(fit_intercept=False)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.scatter(x_train, y_train, color='black', label='training data') #plot training data\n",
    "\n",
    "#fit polynomial models on random samples of training sets\n",
    "S = 100\n",
    "## Step 1: initialize a list to record all the model predections\n",
    "## predictions = ...\n",
    "\n",
    "for i in range(S):\n",
    "    #resample training data\n",
    "    x_train, y_train = make_data(number_of_train_points, f)\n",
    "    #fit our polynomial model on the resampled data\n",
    "    trained_regressor, y_predict = fit_polynomial_regression(x_train, y_train, x_test, polynomial_features_high, regression)\n",
    "\n",
    "    ## Step 2: add new model prediction to list of predictions\n",
    "    ## COMPLETE\n",
    "\n",
    "    #plot the fitted polynomial model\n",
    "    if i == 0:\n",
    "      ax.plot(x_test, y_predict, color='red', alpha=0.1, label='model in ensemble')\n",
    "    else:\n",
    "      ax.plot(x_test, y_predict, color='red', alpha=0.1)\n",
    "\n",
    "## Step 3: average the list of predictions\n",
    "## y_predict_avg = ...\n",
    "\n",
    "## Step 4: visualize average predictions\n",
    "## ax.plot(x_test, y_predict_avg, color='red', linewidth=5, alpha=1., label='average model')\n",
    "\n",
    "ax.set_title('Fitting a high degree polynomial on slightly different data')\n",
    "ax.set_ylim([-2, 2])\n",
    "ax.legend(loc='best')\n",
    "\n",
    "ax.set_title('Fitting an ensemble of high degree polynomials')\n",
    "ax.set_ylim([-2, 2])\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgfobNJ207F9"
   },
   "source": [
    "**Exercise 7:** What do you notice about each of the models in your ensemble? What do you notice about their average? State your answer in terms of variance, uncertainty and other concepts you've learned from lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwOSI_q3Oy4i"
   },
   "source": [
    "---\n",
    "# This section is for Extra Credit, the exercises are optional.\n",
    "## 4. Bootstrap\n",
    "\n",
    "In the provided `make_data` function above, we resampled new training inputs from the interval $[-5, 5]$. When we do not have access to new data (e.g. collecting new data is prohibitively expensive), we need to create \"new data\" from our existing data. To do this, we **bootstrap**, that is, we sample different subsets from our existing data.\n",
    "\n",
    "**Exercise 8 (Extra Credit):** Adapt the `make_data` function to bootstrap sample from an existing dataset `x`, `y`, and create your ensemble using these bootstrap samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sXCD3BS607F9"
   },
   "outputs": [],
   "source": [
    "def bootstrap_data(number_of_train_points, x, y):\n",
    "    '''\n",
    "    This function randomly samples training data from existing data x and y.\n",
    "    \n",
    "    number_of_train_points: number of training points to sample\n",
    "    number_of_test_points: number of testing points to sample\n",
    "    x: array of existing inputs\n",
    "    y: array of existing labels \n",
    "    f: the underlying function relating x and y\n",
    "    '''\n",
    "    # Step 1: randomly select indices in x and y\n",
    "    # Hint: use the numpy.random.choice to choose a set of indices from 0 to the len(x)\n",
    "    # https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.random.choice.html\n",
    "    # random_indices = ...\n",
    "    # \n",
    "    # Step 2: select x values using this set of random indices:\n",
    "    # x_train = ...\n",
    "    # Step 3: select y values using this set of random indices:\n",
    "    # y_train = ...\n",
    "\n",
    "    return x_train, y_train\n",
    "\n",
    "x = np.random.uniform(-5, 5, number_of_train_points)\n",
    "y = f(x) + np.random.normal(0, 0.5, number_of_train_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLC8Keo3wJ3a"
   },
   "source": [
    "After filling in the `bootstrap_data` function. Uncomment the following and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SAzEqZJM07GB"
   },
   "outputs": [],
   "source": [
    "# #instantiate a polynomial feature model for a degree 20 polynomial\n",
    "# polynomial_features_high = PolynomialFeatures(20)\n",
    "\n",
    "# #instantiate a linear regression model\n",
    "# regression = LinearRegression(fit_intercept=False) \n",
    "\n",
    "# # visualize each model in the bootstrap ensemble and visualize their average\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "# ax.scatter(x_train, y_train, color='black') #visualize the training set\n",
    "\n",
    "# y_average_predict = 0\n",
    "# ensemble_size = 100\n",
    "# for i in range(ensemble_size):\n",
    "#     #resample training data\n",
    "#     x_train, y_train = bootstrap_data(number_of_train_points, x, y)\n",
    "#     #fit our polynomial model on the resampled data\n",
    "#     _, y_predict = fit_polynomial_regression(x_train, y_train, x_test, polynomial_features_high, regression)\n",
    "#     #add the prediction of the new model to the predictions of the previous models\n",
    "#     y_average_predict += y_predict\n",
    "#     #plot the function learned for the resampled training data\n",
    "#     ax.plot(x_test, y_predict, color='red', alpha=0.1)\n",
    "\n",
    "# #average the predictions of the models\n",
    "# y_average_predict /= ensemble_size * 1.\n",
    "\n",
    "# #plot the averaged model\n",
    "# ax.plot(x_test, y_average_predict, linewidth=5, color='red', label='average model')\n",
    "# ax.legend(loc='best')\n",
    "# ax.set_title('Fitting an ensemble of high degree polynomials')\n",
    "# ax.set_ylim([-2, 2])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skDMTsKA07GA"
   },
   "source": [
    "**Exercise 9 (Extra Credit)**: What is the difference between building your ensemble from training data resampled from $[-5, 5]$ and training data bootstraped samples from an existing data set $x, y$? Is this what you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JXPT9gM_52A2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "03-regularization-bagging.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
