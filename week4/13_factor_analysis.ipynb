{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13_factor_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDMHeGcjNraT"
      },
      "source": [
        "# DAY 13: Nonnegative Matrix Factorization\n",
        "\n",
        "\n",
        "### Machine Learning and Computational Statistics (DSC6232)\n",
        "\n",
        "#### Instructors: Weiwei Pan, Melanie Fernandez, Pavlos Protopapas\n",
        "\n",
        "#### Due: August 12th, 2:00 pm Kigali Time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reQ30R54OdJ7"
      },
      "source": [
        "**First name**: _________________________________________________________\n",
        "\n",
        "\n",
        "**Last name**: _____________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpRW0Pz0OdoU"
      },
      "source": [
        "## Learning Goals:\n",
        "\n",
        "1. learn how to process and encode text data\n",
        "2. understand how to analyze documents using a simple topic model \n",
        "3. learn how to interpret nonnegative matrix factorization models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXr0ChYk3Suj"
      },
      "source": [
        "### Load necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQjmfrXs3PZ-"
      },
      "source": [
        "import random\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from IPython.display import display\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELmGEE5p3XtF"
      },
      "source": [
        "\n",
        "### We include auxiliary functions here that we will need to use later  **No need to read in details!**\n",
        "\n",
        "We include auxiliary functions here that we will need to use later\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJRD0WyL3Uwz"
      },
      "source": [
        "def print_top_words(model, feature_names, n_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        message = \"Topic #%d: \" % topic_idx\n",
        "        message += \" \".join([feature_names[i]\n",
        "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
        "        print(message)\n",
        "    print()\n",
        "\n",
        "def print_features(feature_names, num_columns=5):\n",
        "    padding = num_columns - len(feature_names) % num_columns\n",
        "    feature_names += [''] * (padding * (padding != num_columns))\n",
        "    feature_names = np.array(feature_names).reshape(-1, num_columns)\n",
        "    display(pd.DataFrame(feature_names, columns=[''] * num_columns).reset_index(drop=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DagRffRp9ADz"
      },
      "source": [
        "# Topic Modeling for News Articles\n",
        "\n",
        "This exercise is designed to help you transform and model textual data. You may find the tutorial [here](http://scikit-learn.org/stable/modules/feature_extraction.html) helpful.\n",
        "\n",
        "You will encode a small set of news articles (i.e. represent them as count vectors) and model this set using a Nonnegative Factorization Model. Your goal is to discover a latent set of topic underlying the articles and discover which topics appear in each article.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugiBG-cL3cFi"
      },
      "source": [
        "### Load-in the data and examine it\n",
        "\n",
        "We use the `fetch_20newsgroups` function from `sklearn` to load a set of articles in the categories: \"medicine\", \"religion\" and \"motorcycles\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIhnx8QK3Zmf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "279397c8-0ee5-44df-87b9-19d3b4e8b6f7"
      },
      "source": [
        "# Load the data set\n",
        "print(\"Loading dataset...\")\n",
        "data, _ = fetch_20newsgroups(shuffle=False, remove=('headers', 'footers', 'quotes'),\n",
        "                             return_X_y=True, categories=['sci.med', 'soc.religion.christian', 'rec.motorcycles'])\n",
        "print('Done.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gB97T9d-wz7"
      },
      "source": [
        "We check to see how many articles we have loaded. We also print two articles form this set to see what they look like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f2Ujdei3gtN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "d9040e3e-d12b-4db7-c60d-c6bf57d2a16f"
      },
      "source": [
        "# Print the number of articles in the data\n",
        "print('Number of data points: {}\\n'.format(len(data)))\n",
        "\n",
        "# Print out an example article from the data\n",
        "print('Example articles:\\n\\n')\n",
        "print('*' * 10 + ' Example 1 ' + '*' * 10)\n",
        "print(data[0])\n",
        "\n",
        "# Print out another article from the data\n",
        "print('\\n\\n' + '*' * 10 + ' Example 2 ' + '*' * 10)\n",
        "print(data[5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of data points: 1791\n",
            "\n",
            "Example articles:\n",
            "\n",
            "\n",
            "********** Example 1 **********\n",
            "Does anyone on this newsgroup happen to know WHY morphine was\n",
            "first isolated from opium?  If you know why, or have an idea for where I\n",
            "could look to find this info, please mail me.\n",
            "\tCSH\n",
            "any suggestionas would be greatly appreciated\n",
            "\n",
            "--\n",
            " \"Kilimanjaro is a pretty tricky climb. Most of it's up, until you reach\n",
            "the very, very top, and then it tends to slope away rather sharply.\"\n",
            "\t\t\t\t\tSir George Head, OBE (JC)\n",
            "\n",
            "\n",
            "********** Example 2 **********\n",
            "I just noticed that my halogen table lamp runs off 12 Volts.\n",
            "The big thinngy that plugs into the wall says 12 Volts DC,  20mA\n",
            "\n",
            "The question is: Can I trickle charge the battery on my CB650\n",
            "with it?\n",
            "\n",
            "I don't know the rating of the battery, but it is a factory\n",
            "intalled one. \n",
            "\n",
            "\n",
            "Thanks,\n",
            "Sanjay\n",
            "\n",
            "-- \n",
            "   '81 CB650 \t\t\t\t\t\tDoD #1224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzOpUB-cT7GR"
      },
      "source": [
        "### Encode the data as count vectors\n",
        "\n",
        "We are going to use `sklearn`'s `TfidfVectorizer` function to remove punctuations and non-meaningful words from the documents and then convert them into count vectors.\n",
        "\n",
        "**Exercise 1:** Read the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) for `TfidfVectorizer`, and experiment with different values for the parameters `max_df`, `min_df`, `max_features`. What do each of these parameters mean? How does changing these parameters change the count vector representation of the data?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XEBy0iz3jgp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        },
        "outputId": "9ecd1b7e-e6f1-49a6-ba87-728b372258e8"
      },
      "source": [
        "# Step 1: Reduce the size of the data\n",
        "n_samples = 1000\n",
        "data_samples = random.sample(data, min(n_samples, len(data)))\n",
        "\n",
        "# Step 2: Choose the number of features, or important words, to extract\n",
        "n_features = 1000\n",
        "\n",
        "# Step 3: Extract tf-idf features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=n_features, stop_words='english')\n",
        "\n",
        "# Step 4: Encode the documents as normalized count vectors\n",
        "vectorized_data = tfidf_vectorizer.fit_transform(data_samples)\n",
        "\n",
        "# Step 5: Get learned feature names\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
        "\n",
        "# Step 6: Select a samplee of the learned features \n",
        "sample_of_features = random.sample(tfidf_feature_names, 100)\n",
        "\n",
        "# Step 7: Print that sample of learned feature names\n",
        "print_features(sample_of_features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>history</td>\n",
              "      <td>moral</td>\n",
              "      <td>relationship</td>\n",
              "      <td>let</td>\n",
              "      <td>management</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>number</td>\n",
              "      <td>natural</td>\n",
              "      <td>liver</td>\n",
              "      <td>hey</td>\n",
              "      <td>happen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>certainly</td>\n",
              "      <td>context</td>\n",
              "      <td>problems</td>\n",
              "      <td>organization</td>\n",
              "      <td>case</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>christian</td>\n",
              "      <td>feeling</td>\n",
              "      <td>center</td>\n",
              "      <td>candida</td>\n",
              "      <td>called</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>knew</td>\n",
              "      <td>key</td>\n",
              "      <td>software</td>\n",
              "      <td>word</td>\n",
              "      <td>causes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>opinion</td>\n",
              "      <td>sp</td>\n",
              "      <td>gift</td>\n",
              "      <td>finding</td>\n",
              "      <td>school</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>valid</td>\n",
              "      <td>levels</td>\n",
              "      <td>oil</td>\n",
              "      <td>purpose</td>\n",
              "      <td>today</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>law</td>\n",
              "      <td>according</td>\n",
              "      <td>data</td>\n",
              "      <td>friend</td>\n",
              "      <td>cadre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>kept</td>\n",
              "      <td>later</td>\n",
              "      <td>look</td>\n",
              "      <td>guess</td>\n",
              "      <td>works</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>james</td>\n",
              "      <td>does</td>\n",
              "      <td>numbers</td>\n",
              "      <td>peace</td>\n",
              "      <td>authority</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>change</td>\n",
              "      <td>accepted</td>\n",
              "      <td>special</td>\n",
              "      <td>doctors</td>\n",
              "      <td>directly</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>black</td>\n",
              "      <td>access</td>\n",
              "      <td>60</td>\n",
              "      <td>tried</td>\n",
              "      <td>humans</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>doesn</td>\n",
              "      <td>mother</td>\n",
              "      <td>reply</td>\n",
              "      <td>mice</td>\n",
              "      <td>000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>area</td>\n",
              "      <td>month</td>\n",
              "      <td>gone</td>\n",
              "      <td>getting</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>rec</td>\n",
              "      <td>come</td>\n",
              "      <td>culture</td>\n",
              "      <td>hear</td>\n",
              "      <td>standard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>local</td>\n",
              "      <td>course</td>\n",
              "      <td>ride</td>\n",
              "      <td>bikes</td>\n",
              "      <td>turned</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>teachings</td>\n",
              "      <td>miles</td>\n",
              "      <td>email</td>\n",
              "      <td>admit</td>\n",
              "      <td>choose</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>date</td>\n",
              "      <td>letter</td>\n",
              "      <td>living</td>\n",
              "      <td>died</td>\n",
              "      <td>assume</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>internet</td>\n",
              "      <td>rear</td>\n",
              "      <td>research</td>\n",
              "      <td>far</td>\n",
              "      <td>aren</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>knowledge</td>\n",
              "      <td>story</td>\n",
              "      <td>weeks</td>\n",
              "      <td>effective</td>\n",
              "      <td>honda</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                \n",
              "0     history      moral  relationship           let  management\n",
              "1      number    natural         liver           hey      happen\n",
              "2   certainly    context      problems  organization        case\n",
              "3   christian    feeling        center       candida      called\n",
              "4        knew        key      software          word      causes\n",
              "5     opinion         sp          gift       finding      school\n",
              "6       valid     levels           oil       purpose       today\n",
              "7         law  according          data        friend       cadre\n",
              "8        kept      later          look         guess       works\n",
              "9       james       does       numbers         peace   authority\n",
              "10     change   accepted       special       doctors    directly\n",
              "11      black     access            60         tried      humans\n",
              "12      doesn     mother         reply          mice         000\n",
              "13       area      month          gone       getting          23\n",
              "14        rec       come       culture          hear    standard\n",
              "15      local     course          ride         bikes      turned\n",
              "16  teachings      miles         email         admit      choose\n",
              "17       date     letter        living          died      assume\n",
              "18   internet       rear      research           far        aren\n",
              "19  knowledge      story         weeks     effective       honda"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_mniF8UV7_T"
      },
      "source": [
        "**Exercise 2:** Print the normalized count vector representation of a single document. What kind of numbers are in this vector? What do these numbers represent? ***Hint:*** recall how we process count vectors before fitting a nonnegative matrix factorization model. \n",
        "\n",
        "The `TfidfVectorizer` function normalizes the count vectors, what does this mean and why is this step necessary?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WeIyRJoVfVI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ed48a7b1-cb6a-4f68-a404-e6376ca31ddf"
      },
      "source": [
        "# Step 1: Print the normalized count vector representation of a single document\n",
        "n = 10\n",
        "print('The normalized ount vector representation of the {}-th document'.format(n))\n",
        "print(vectorized_data[n])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The normalized ount vector representation of the 10-th document\n",
            "  (0, 469)\t0.6544542871507429\n",
            "  (0, 909)\t0.3461309091439971\n",
            "  (0, 940)\t0.5665912234638764\n",
            "  (0, 395)\t0.36174212535848044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ_7F2kpWoLr"
      },
      "source": [
        "### Fit a Nonnegative Matrix Factorization Model to the data\n",
        "\n",
        "Now that our data has been encoded as normalized count vectors, we can fit an NMF model to it.\n",
        "\n",
        "**Exercise 3:** Fit an NMF model with 10 topics, print out the top words associated to each topic. Can you interpret what each topic is about?\n",
        "\n",
        "Fit an NMF model with 2 topics, print out the top words associated to each topic. Can you interpret what each topic is about?\n",
        "\n",
        "Find an appropriate number of topics. Why is this number appropriate?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VUYYgtt3lKI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "70a8ccaa-aed8-41e9-f0d3-8061eb422fbb"
      },
      "source": [
        "# Step 1: Fit NMF model\n",
        "nmf = NMF(n_components=10, alpha=0.1, l1_ratio=0.5).fit(vectorized_data)\n",
        "\n",
        "# Step 2: Print out the learned topics\n",
        "print('Topics learned by the NMF:')\n",
        "print_top_words(nmf, tfidf_feature_names, 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topics learned by the NMF:\n",
            "Topic #0: don like know just think time good people really ve\n",
            "Topic #1: banks geb pitt shameful n3jxp cadre intellect dsl chastity skepticism\n",
            "Topic #2: bike bikes ride dod motorcycle miles honda helmet riding buy\n",
            "Topic #3: msg food chinese eat brain foods effects natural study sick\n",
            "Topic #4: god lord christ heaven son truth life believe sin bible\n",
            "Topic #5: dog dogs bikers road running said large human face head\n",
            "Topic #6: church catholic pope authority bishop marriage married orthodox schism st\n",
            "Topic #7: jesus father christ people christian knew come black apostles dead\n",
            "Topic #8: disease candida patients yeast diseases body medical 90 syndrome lyme\n",
            "Topic #9: right left turn countersteering eye neck rider riding close feet\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9wU5R5nXdRq"
      },
      "source": [
        "**Exercise 4:** Pick a document and print out the combinations of topics in that document. Which combinations of topics are contained in this article? Do you agree with the the combination of topics learned by the model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZeLGjPMDjiD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9433e1f3-62c7-4ed1-d3c1-16d4a27208ca"
      },
      "source": [
        "# Step 1: get the document to topic matrix each row of this matrix is the combination of topics in the document\n",
        "document_to_topic = nmf.transform(vectorized_data)\n",
        "\n",
        "# Step 2: print the shape of this matrix to verify that we have 1000 documents and 10 topic\n",
        "print(document_to_topic.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF81swCccOOa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "31642949-7357-4fad-9b44-01b3d5030544"
      },
      "source": [
        "# Step 3: print an article and the predicted combination of topics in this article\n",
        "n = 10\n",
        "\n",
        "# Print the predicted combination of topics in this article\n",
        "print('\\n\\n' + '*' * 10 + ' Predicted combination of topics ' + '*' * 10 + '\\n\\n')\n",
        "print(document_to_topic[n])\n",
        "\n",
        "# Print out an example article from the data\n",
        "print('\\n\\n' + '*' * 10 + ' Article {} '.format(n) + '*' * 10)\n",
        "print(data[n])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "********** Predicted combination of topics **********\n",
            "\n",
            "\n",
            "[0.04818951 0.         0.0093347  0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n",
            "\n",
            "\n",
            "********** Article 10 **********\n",
            "\n",
            "I attribute my success to several factors:\n",
            "\n",
            "Very low fat.  Except when someone else has cooked a meal for me,\n",
            "I only eat fruit, vegetables, and whole grain or bran cereals.  I\n",
            "estimate I only get about 5 to 10 percent of my calories from fat.\n",
            "\n",
            "Very little sugar or salt.\n",
            "\n",
            "Very high fiber.  Most Americans get about 10 grams.  25 to 35 are\n",
            "recommended.  I get between 50 and 150.  Sometimes 200.  (I've heard\n",
            "of people taking fiber pills.  It seems unlikely that pills can\n",
            "contain enough fiber to make a difference.  It would be about as\n",
            "likely as someone getting fat by popping fat pills.  Tablets are\n",
            "just too small, unless you snarf down hundreds of them daily.)\n",
            "\n",
            "My \"clean your plate\" conditioning works *for* me.  Eating the last\n",
            "10% takes half my eating time, and gives satiety a chance to catch\n",
            "up, so I don't still feel hungry and go start eating something else.\n",
            "\n",
            "I don't eat when I'm not hungry (unless I'm sure I'll get hungry\n",
            "shortly, and eating won't be practical then).\n",
            "\n",
            "I bike to work, 22 miles a day, year round.  Fast.  I also bike to\n",
            "stores, movies, and everywhere else, as I've never owned a car.\n",
            "I estimate this burns about 1000 calories a day.  It also helps\n",
            "build and maintain muscle mass, prevent insulin resistance (diabetes\n",
            "runs in my family), and increase my metabolism.  (Even so, my\n",
            "metabolism is so low that when I'm at rest I'm most comfortable\n",
            "with a temperature in the 90s (F), and usually wear a sweater if\n",
            "it drops to 80.)  Cycling also motivates me to avoid every excess\n",
            "ounce.  (Cyclists routinely pay a premium for cycling products that\n",
            "weigh slightly less than others.  But it's easier and cheaper to trim\n",
            "weight from the rider than from the vehicle.)\n",
            "\n",
            "There's no question in my mind that my metabolism is radically\n",
            "different from that of most people who have never been fat.  Fortunately,\n",
            "it isn't different in a way that precludes excellent health.\n",
            "\n",
            "Obviously, I can't swear that every obese person who does what I've\n",
            "done will have the success I did.  But I've never yet heard of one who\n",
            "did try it and didn't succeed.\n",
            "\n",
            "\n",
            "I'm sure everyone's weight cycles, whether or not they've ever been fat.\n",
            "I usually eat extremely little salt.  When I do eat something salty,\n",
            "my weight can increase overnight by as much as ten pounds.  It comes\n",
            "off again over a week or two.\n",
            "-- \n",
            "Keith Lynch, kfl@access.digex.com\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R9CgwwiDXVX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}