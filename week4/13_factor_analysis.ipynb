{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDMHeGcjNraT"
   },
   "source": [
    "# DAY 13: Nonnegative Matrix Factorization\n",
    "\n",
    "\n",
    "### Machine Learning and Computational Statistics (DSC6232)\n",
    "\n",
    "#### Instructors: Weiwei Pan, Melanie Fernandez, Pavlos Protopapas\n",
    "\n",
    "#### Due: August 12th, 2:00 pm Kigali Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reQ30R54OdJ7"
   },
   "source": [
    "**First name**: _________________________________________________________\n",
    "\n",
    "\n",
    "**Last name**: _____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpRW0Pz0OdoU"
   },
   "source": [
    "## Learning Goals:\n",
    "\n",
    "1. learn how to process and encode text data\n",
    "2. understand how to analyze documents using a simple topic model \n",
    "3. learn how to interpret nonnegative matrix factorization models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXr0ChYk3Suj"
   },
   "source": [
    "### Load necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DQjmfrXs3PZ-"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from IPython.display import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELmGEE5p3XtF"
   },
   "source": [
    "\n",
    "### We include auxiliary functions here that we will need to use later  **No need to read in details!**\n",
    "\n",
    "We include auxiliary functions here that we will need to use later\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MJRD0WyL3Uwz"
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "def print_features(feature_names, num_columns=5):\n",
    "    padding = num_columns - len(feature_names) % num_columns\n",
    "    feature_names += [''] * (padding * (padding != num_columns))\n",
    "    feature_names = np.array(feature_names).reshape(-1, num_columns)\n",
    "    display(pd.DataFrame(feature_names, columns=[''] * num_columns).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DagRffRp9ADz"
   },
   "source": [
    "# Topic Modeling for News Articles\n",
    "\n",
    "This exercise is designed to help you transform and model textual data. You may find the tutorial [here](http://scikit-learn.org/stable/modules/feature_extraction.html) helpful.\n",
    "\n",
    "You will encode a small set of news articles (i.e. represent them as count vectors) and model this set using a Nonnegative Factorization Model. Your goal is to discover a latent set of topic underlying the articles and discover which topics appear in each article.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugiBG-cL3cFi"
   },
   "source": [
    "### Load-in the data and examine it\n",
    "\n",
    "We use the `fetch_20newsgroups` function from `sklearn` to load a set of articles in the categories: \"medicine\", \"religion\" and \"motorcycles\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "DIhnx8QK3Zmf",
    "outputId": "279397c8-0ee5-44df-87b9-19d3b4e8b6f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Load the data set\n",
    "print(\"Loading dataset...\")\n",
    "data, _ = fetch_20newsgroups(shuffle=False, remove=('headers', 'footers', 'quotes'),\n",
    "                             return_X_y=True, categories=['sci.med', 'soc.religion.christian', 'rec.motorcycles'])\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gB97T9d-wz7"
   },
   "source": [
    "We check to see how many articles we have loaded. We also print two articles form this set to see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "id": "4f2Ujdei3gtN",
    "outputId": "d9040e3e-d12b-4db7-c60d-c6bf57d2a16f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points: 1791\n",
      "\n",
      "Example articles:\n",
      "\n",
      "\n",
      "********** Example 1 **********\n",
      "Does anyone on this newsgroup happen to know WHY morphine was\n",
      "first isolated from opium?  If you know why, or have an idea for where I\n",
      "could look to find this info, please mail me.\n",
      "\tCSH\n",
      "any suggestionas would be greatly appreciated\n",
      "\n",
      "--\n",
      " \"Kilimanjaro is a pretty tricky climb. Most of it's up, until you reach\n",
      "the very, very top, and then it tends to slope away rather sharply.\"\n",
      "\t\t\t\t\tSir George Head, OBE (JC)\n",
      "\n",
      "\n",
      "********** Example 2 **********\n",
      "I just noticed that my halogen table lamp runs off 12 Volts.\n",
      "The big thinngy that plugs into the wall says 12 Volts DC,  20mA\n",
      "\n",
      "The question is: Can I trickle charge the battery on my CB650\n",
      "with it?\n",
      "\n",
      "I don't know the rating of the battery, but it is a factory\n",
      "intalled one. \n",
      "\n",
      "\n",
      "Thanks,\n",
      "Sanjay\n",
      "\n",
      "-- \n",
      "   '81 CB650 \t\t\t\t\t\tDoD #1224\n"
     ]
    }
   ],
   "source": [
    "# Print the number of articles in the data\n",
    "print('Number of data points: {}\\n'.format(len(data)))\n",
    "\n",
    "# Print out an example article from the data\n",
    "print('Example articles:\\n\\n')\n",
    "print('*' * 10 + ' Example 1 ' + '*' * 10)\n",
    "print(data[0])\n",
    "\n",
    "# Print out another article from the data\n",
    "print('\\n\\n' + '*' * 10 + ' Example 2 ' + '*' * 10)\n",
    "print(data[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzOpUB-cT7GR"
   },
   "source": [
    "### Encode the data as count vectors\n",
    "\n",
    "We are going to use `sklearn`'s `TfidfVectorizer` function to remove punctuations and non-meaningful words from the documents and then convert them into count vectors.\n",
    "\n",
    "**Exercise 1:** Read the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) for `TfidfVectorizer`, and experiment with different values for the parameters `max_df`, `min_df`, `max_features`. What do each of these parameters mean? How does changing these parameters change the count vector representation of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 652
    },
    "id": "1XEBy0iz3jgp",
    "outputId": "9ecd1b7e-e6f1-49a6-ba87-728b372258e8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>years</td>\n",
       "      <td>software</td>\n",
       "      <td>tell</td>\n",
       "      <td>patient</td>\n",
       "      <td>war</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1993</td>\n",
       "      <td>didn</td>\n",
       "      <td>fit</td>\n",
       "      <td>exist</td>\n",
       "      <td>drug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>know</td>\n",
       "      <td>subject</td>\n",
       "      <td>world</td>\n",
       "      <td>trying</td>\n",
       "      <td>try</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>advice</td>\n",
       "      <td>questions</td>\n",
       "      <td>motorcycle</td>\n",
       "      <td>available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>suppose</td>\n",
       "      <td>result</td>\n",
       "      <td>right</td>\n",
       "      <td>instead</td>\n",
       "      <td>hurt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kind</td>\n",
       "      <td>st</td>\n",
       "      <td>page</td>\n",
       "      <td>matter</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>says</td>\n",
       "      <td>self</td>\n",
       "      <td>skepticism</td>\n",
       "      <td>pain</td>\n",
       "      <td>experience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>list</td>\n",
       "      <td>mind</td>\n",
       "      <td>money</td>\n",
       "      <td>second</td>\n",
       "      <td>specific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>christianity</td>\n",
       "      <td>11</td>\n",
       "      <td>able</td>\n",
       "      <td>car</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>actually</td>\n",
       "      <td>come</td>\n",
       "      <td>interesting</td>\n",
       "      <td>foods</td>\n",
       "      <td>rec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>start</td>\n",
       "      <td>works</td>\n",
       "      <td>ideas</td>\n",
       "      <td>bad</td>\n",
       "      <td>cadre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>make</td>\n",
       "      <td>argument</td>\n",
       "      <td>life</td>\n",
       "      <td>opinion</td>\n",
       "      <td>sure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>just</td>\n",
       "      <td>wanted</td>\n",
       "      <td>men</td>\n",
       "      <td>used</td>\n",
       "      <td>getting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>knowing</td>\n",
       "      <td>david</td>\n",
       "      <td>soon</td>\n",
       "      <td>grace</td>\n",
       "      <td>sort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>source</td>\n",
       "      <td>thanks</td>\n",
       "      <td>water</td>\n",
       "      <td>engine</td>\n",
       "      <td>school</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>earth</td>\n",
       "      <td>street</td>\n",
       "      <td>volume</td>\n",
       "      <td>example</td>\n",
       "      <td>stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>open</td>\n",
       "      <td>year</td>\n",
       "      <td>religion</td>\n",
       "      <td>natural</td>\n",
       "      <td>place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>related</td>\n",
       "      <td>ve</td>\n",
       "      <td>like</td>\n",
       "      <td>medicine</td>\n",
       "      <td>clinical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>jewish</td>\n",
       "      <td>dsl</td>\n",
       "      <td>perfect</td>\n",
       "      <td>man</td>\n",
       "      <td>heard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>assume</td>\n",
       "      <td>strong</td>\n",
       "      <td>maybe</td>\n",
       "      <td>n3jxp</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               \n",
       "0          years  software         tell     patient         war\n",
       "1           1993      didn          fit       exist        drug\n",
       "2           know   subject        world      trying         try\n",
       "3            dog    advice    questions  motorcycle   available\n",
       "4        suppose    result        right     instead        hurt\n",
       "5           kind        st         page      matter   condition\n",
       "6           says      self   skepticism        pain  experience\n",
       "7           list      mind        money      second    specific\n",
       "8   christianity        11         able         car        best\n",
       "9       actually      come  interesting       foods         rec\n",
       "10         start     works        ideas         bad       cadre\n",
       "11          make  argument         life     opinion        sure\n",
       "12          just    wanted          men        used     getting\n",
       "13       knowing     david         soon       grace        sort\n",
       "14        source    thanks        water      engine      school\n",
       "15         earth    street       volume     example       stuff\n",
       "16          open      year     religion     natural       place\n",
       "17       related        ve         like    medicine    clinical\n",
       "18        jewish       dsl      perfect         man       heard\n",
       "19        assume    strong        maybe       n3jxp        said"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Reduce the size of the data\n",
    "n_samples = 1000\n",
    "data_samples = random.sample(data, min(n_samples, len(data)))\n",
    "\n",
    "# Step 2: Choose the number of features, or important words, to extract\n",
    "n_features = 500\n",
    "\n",
    "# Step 3: Extract tf-idf features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=5, max_features=n_features, stop_words='english')\n",
    "\n",
    "# Step 4: Encode the documents as normalized count vectors\n",
    "vectorized_data = tfidf_vectorizer.fit_transform(data_samples)\n",
    "\n",
    "# Step 5: Get learned feature names\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Step 6: Select a samplee of the learned features \n",
    "sample_of_features = random.sample(sorted(tfidf_feature_names), 100)\n",
    "\n",
    "# Step 7: Print that sample of learned feature names\n",
    "print_features(sample_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_mniF8UV7_T"
   },
   "source": [
    "**Exercise 2:** Print the normalized count vector representation of a single document. What kind of numbers are in this vector? What do these numbers represent? ***Hint:*** recall how we process count vectors before fitting a nonnegative matrix factorization model. \n",
    "\n",
    "The `TfidfVectorizer` function normalizes the count vectors, what does this mean and why is this step necessary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "1WeIyRJoVfVI",
    "outputId": "ed48a7b1-cb6a-4f68-a404-e6376ca31ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The normalized count vector representation of the 10-th document\n",
      "  (0, 117)\t0.11305800827015901\n",
      "  (0, 233)\t0.17937797725929913\n",
      "  (0, 313)\t0.1559434312822325\n",
      "  (0, 140)\t0.15791746432932727\n",
      "  (0, 237)\t0.17427516845032495\n",
      "  (0, 364)\t0.17589813420027145\n",
      "  (0, 87)\t0.3045786247813023\n",
      "  (0, 225)\t0.1514301970447727\n",
      "  (0, 193)\t0.16337697524024486\n",
      "  (0, 94)\t0.15059083323716208\n",
      "  (0, 123)\t0.18988145616452512\n",
      "  (0, 84)\t0.1321065592771092\n",
      "  (0, 44)\t0.10938975710915493\n",
      "  (0, 443)\t0.12098318912139533\n",
      "  (0, 231)\t0.25483854151349056\n",
      "  (0, 18)\t0.13634443754267764\n",
      "  (0, 497)\t0.1142650840757048\n",
      "  (0, 13)\t0.1514301970447727\n",
      "  (0, 494)\t0.3368418864691065\n",
      "  (0, 329)\t0.11883488024174688\n",
      "  (0, 268)\t0.10755729352054061\n",
      "  (0, 352)\t0.11305800827015901\n",
      "  (0, 250)\t0.08545005863365784\n",
      "  (0, 130)\t0.10145802901185105\n",
      "  (0, 251)\t0.15407067629618976\n",
      "  (0, 134)\t0.08760259060250164\n",
      "  (0, 405)\t0.13988049418878795\n",
      "  (0, 406)\t0.17589813420027145\n",
      "  (0, 298)\t0.2285301681514096\n",
      "  (0, 55)\t0.4196414825663638\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Print the normalized count vector representation of a single document\n",
    "n = 10\n",
    "print('The normalized count vector representation of the {}-th document'.format(n))\n",
    "print(vectorized_data[n, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZ_7F2kpWoLr"
   },
   "source": [
    "### Fit a Nonnegative Matrix Factorization Model to the data\n",
    "\n",
    "Now that our data has been encoded as normalized count vectors, we can fit an NMF model to it.\n",
    "\n",
    "**Exercise 3:** Fit an NMF model with 10 topics, print out the top words associated to each topic. Can you interpret what each topic is about?\n",
    "\n",
    "Fit an NMF model with 2 topics, print out the top words associated to each topic. Can you interpret what each topic is about?\n",
    "\n",
    "Find an appropriate number of topics. Why is this number appropriate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "5VUYYgtt3lKI",
    "outputId": "70a8ccaa-aed8-41e9-f0d3-8061eb422fbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics learned by the NMF:\n",
      "Topic #0: god love life lord sin hell scripture heaven christ faith\n",
      "Topic #1: geb banks pitt chastity n3jxp dsl shameful cadre surrender skepticism\n",
      "Topic #2: bike bikes dod ride riding motorcycle miles engine turn advice\n",
      "Topic #3: doctor pain years time patients disease new treatment ago problems\n",
      "Topic #4: msg food eat use foods people effects effect cause natural\n",
      "Topic #5: thanks know edu info does looking mail information email post\n",
      "Topic #6: believe truth people science true christians does think christianity question\n",
      "Topic #7: jesus church christ mary say father john pope did son\n",
      "Topic #8: don just like think ll know people ve little try\n",
      "Topic #9: helmet fall head just fit long face usually ground big\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krosenfeld\\Documents\\projects\\other\\Rwanda-Data-Science-NBs\\envs\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1422: FutureWarning: `alpha` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\krosenfeld\\Documents\\projects\\other\\Rwanda-Data-Science-NBs\\envs\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1432: FutureWarning: `regularization` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Fit NMF model\n",
    "nmf = NMF(n_components=10, alpha=0.1, l1_ratio=0.5, regularization=None, init='nndsvd').fit(vectorized_data)\n",
    "\n",
    "# Step 2: Print out the learned topics\n",
    "print('Topics learned by the NMF:')\n",
    "print_top_words(nmf, tfidf_feature_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9wU5R5nXdRq"
   },
   "source": [
    "**Exercise 4:** Pick a document and print out the combinations of topics in that document. Which combinations of topics are contained in this article? Do you agree with the the combination of topics learned by the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "cZeLGjPMDjiD",
    "outputId": "9433e1f3-62c7-4ed1-d3c1-16d4a27208ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krosenfeld\\Documents\\projects\\other\\Rwanda-Data-Science-NBs\\envs\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1422: FutureWarning: `alpha` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\krosenfeld\\Documents\\projects\\other\\Rwanda-Data-Science-NBs\\envs\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1432: FutureWarning: `regularization` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 1: get the document to topic matrix each row of this matrix is the combination of topics in the document\n",
    "document_to_topic = nmf.transform(vectorized_data)\n",
    "\n",
    "# Step 2: print the shape of this matrix to verify that we have 1000 documents and 10 topic\n",
    "print(document_to_topic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UF81swCccOOa",
    "outputId": "31642949-7357-4fad-9b44-01b3d5030544"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********** Predicted combination of topics **********\n",
      "\n",
      "\n",
      "[0.         0.00134814 0.00029046 0.02690346 0.00311581 0.00057397\n",
      " 0.0566424  0.17742877 0.039824   0.        ]\n",
      "\n",
      "\n",
      "********** Article 10 **********\n",
      "\n",
      "I attribute my success to several factors:\n",
      "\n",
      "Very low fat.  Except when someone else has cooked a meal for me,\n",
      "I only eat fruit, vegetables, and whole grain or bran cereals.  I\n",
      "estimate I only get about 5 to 10 percent of my calories from fat.\n",
      "\n",
      "Very little sugar or salt.\n",
      "\n",
      "Very high fiber.  Most Americans get about 10 grams.  25 to 35 are\n",
      "recommended.  I get between 50 and 150.  Sometimes 200.  (I've heard\n",
      "of people taking fiber pills.  It seems unlikely that pills can\n",
      "contain enough fiber to make a difference.  It would be about as\n",
      "likely as someone getting fat by popping fat pills.  Tablets are\n",
      "just too small, unless you snarf down hundreds of them daily.)\n",
      "\n",
      "My \"clean your plate\" conditioning works *for* me.  Eating the last\n",
      "10% takes half my eating time, and gives satiety a chance to catch\n",
      "up, so I don't still feel hungry and go start eating something else.\n",
      "\n",
      "I don't eat when I'm not hungry (unless I'm sure I'll get hungry\n",
      "shortly, and eating won't be practical then).\n",
      "\n",
      "I bike to work, 22 miles a day, year round.  Fast.  I also bike to\n",
      "stores, movies, and everywhere else, as I've never owned a car.\n",
      "I estimate this burns about 1000 calories a day.  It also helps\n",
      "build and maintain muscle mass, prevent insulin resistance (diabetes\n",
      "runs in my family), and increase my metabolism.  (Even so, my\n",
      "metabolism is so low that when I'm at rest I'm most comfortable\n",
      "with a temperature in the 90s (F), and usually wear a sweater if\n",
      "it drops to 80.)  Cycling also motivates me to avoid every excess\n",
      "ounce.  (Cyclists routinely pay a premium for cycling products that\n",
      "weigh slightly less than others.  But it's easier and cheaper to trim\n",
      "weight from the rider than from the vehicle.)\n",
      "\n",
      "There's no question in my mind that my metabolism is radically\n",
      "different from that of most people who have never been fat.  Fortunately,\n",
      "it isn't different in a way that precludes excellent health.\n",
      "\n",
      "Obviously, I can't swear that every obese person who does what I've\n",
      "done will have the success I did.  But I've never yet heard of one who\n",
      "did try it and didn't succeed.\n",
      "\n",
      "\n",
      "I'm sure everyone's weight cycles, whether or not they've ever been fat.\n",
      "I usually eat extremely little salt.  When I do eat something salty,\n",
      "my weight can increase overnight by as much as ten pounds.  It comes\n",
      "off again over a week or two.\n",
      "-- \n",
      "Keith Lynch, kfl@access.digex.com\n"
     ]
    }
   ],
   "source": [
    "# Step 3: print an article and the predicted combination of topics in this article\n",
    "n = 10\n",
    "\n",
    "# Print the predicted combination of topics in this article\n",
    "print('\\n\\n' + '*' * 10 + ' Predicted combination of topics ' + '*' * 10 + '\\n\\n')\n",
    "print(document_to_topic[n])\n",
    "\n",
    "# Print out an example article from the data\n",
    "print('\\n\\n' + '*' * 10 + ' Article {} '.format(n) + '*' * 10)\n",
    "print(data[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7R9CgwwiDXVX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "13_factor_analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
